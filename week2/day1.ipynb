{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with them through their APIs.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a git pull and merge your changes as needed</a>. Check out the GitHub guide for instructions. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys - OPTIONAL!\n",
    "\n",
    "We're now going to try asking a bunch of models some questions!\n",
    "\n",
    "This is totally optional. If you have keys to Anthropic, Gemini or others, then you can add them in.\n",
    "\n",
    "If you'd rather not spend the extra, then just watch me do it!\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://aistudio.google.com/   \n",
    "For DeepSeek, visit https://platform.deepseek.com/  \n",
    "For Groq, visit https://console.groq.com/  \n",
    "For Grok, visit https://console.x.ai/  \n",
    "\n",
    "\n",
    "You can also use OpenRouter as your one-stop-shop for many of these! OpenRouter is \"the unified interface for LLMs\":\n",
    "\n",
    "For OpenRouter, visit https://openrouter.ai/  \n",
    "\n",
    "\n",
    "With each of the above, you typically have to navigate to:\n",
    "1. Their billing page to add the minimum top-up (except Gemini, Groq, Google, OpenRouter may have free tiers)\n",
    "2. Their API key page to collect your API key\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "GROQ_API_KEY=xxxx\n",
    "GROK_API_KEY=xxxx\n",
    "OPENROUTER_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Any time you change your .env file</h2>\n",
    "            <span style=\"color:#900;\">Remember to Save it! And also rerun load_dotenv(override=True)<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0abffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key not set (and this is optional)\n",
      "Google API Key exists and begins AI\n",
      "DeepSeek API Key not set (and this is optional)\n",
      "Groq API Key not set (and this is optional)\n",
      "Grok API Key not set (and this is optional)\n",
      "OpenRouter API Key not set (and this is optional)\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "grok_api_key = os.getenv('GROK_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")\n",
    "\n",
    "if grok_api_key:\n",
    "    print(f\"Grok API Key exists and begins {grok_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Grok API Key not set (and this is optional)\")\n",
    "\n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set (and this is optional)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "985a859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI client library\n",
    "# A thin wrapper around calls to HTTP endpoints\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# For Gemini, DeepSeek and Groq, we can use the OpenAI python client\n",
    "# Because Google and DeepSeek have endpoints compatible with OpenAI\n",
    "# And OpenAI allows you to change the base_url\n",
    "\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "deepseek_url = \"https://api.deepseek.com\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "grok_url = \"https://api.x.ai/v1\"\n",
    "openrouter_url = \"https://openrouter.ai/api/v1\"\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=deepseek_url)\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=groq_url)\n",
    "grok = OpenAI(api_key=grok_api_key, base_url=grok_url)\n",
    "openrouter = OpenAI(base_url=openrouter_url, api_key=openrouter_api_key)\n",
    "ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16813180",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell_a_joke = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke for a student on the journey to becoming an expert in LLM Engineering\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23e92304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are a few you can use — pick the vibe you want:\n",
       "\n",
       "1) Why did the LLM student carry a ladder to class? Because they heard they needed to get to the next layer to become an expert.\n",
       "\n",
       "2) Student: “I finally fine-tuned my model!”  \n",
       "   Mentor: “Congrats — did you fix its confidence?”  \n",
       "   Student: “Not yet. Now it’s just confidently wrong with better grammar.”\n",
       "\n",
       "3) Why did the aspiring LLM engineer bring coffee to the training run? To keep their own gradients from vanishing.\n",
       "\n",
       "4) How do you know an LLM student is getting serious? They stopped blaming hallucinations and started writing better prompts.\n",
       "\n",
       "5) “What’s the difference between a student and an expert LLM engineer?”  \n",
       "   “The student asks for more compute; the expert asks for better data (and still blames the learning rate).”\n",
       "\n",
       "Want one tailored to a particular stage (data, prompt tuning, inference) or in a different style?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-mini\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e03c11b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the LLM engineer break up with their dataset?\n",
       "\n",
       "Because it had too many *dependencies* and kept *hallucinating* about their future together!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini.chat.completions.create(model=\"gemini-2.5-flash-lite\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ea76a",
   "metadata": {},
   "source": [
    "## Training vs Inference time scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afe9e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": \n",
    "        \"You toss 2 coins. One of them is heads. What's the probability the other is tails? Answer with the probability only.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": null,
   "id": "63230373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> 57dafbaddb8fb3097e6e52b6541731fa149267d6
   "id": "4a887eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1/2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=easy_puzzle, reasoning_effort=\"minimal\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f854d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2/3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=easy_puzzle, reasoning_effort=\"low\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f45fc55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2/3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-mini\", messages=easy_puzzle, reasoning_effort=\"minimal\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca713a5c",
   "metadata": {},
   "source": [
    "## Testing out the best models on the planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df1e825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard = \"\"\"\n",
    "On a bookshelf, two volumes of Pushkin stand side by side: the first and the second.\n",
    "The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick.\n",
    "A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume.\n",
    "What distance did it gnaw through?\n",
    "\"\"\"\n",
    "hard_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": hard}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f6a7827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Interpretation:\n",
       "- Each volume has pages total thickness 2 cm.\n",
       "- Each cover thickness: 2 mm = 0.2 cm.\n",
       "- The two volumes are on a shelf side by side in order: [Volume 1] [Volume 2].\n",
       "- A worm gnaws perpendicular to the pages, starting at the first page of Volume 1 and ending at the last page of Volume 2.\n",
       "\n",
       "What does “first page of the first volume” mean in this setup? Books on a shelf are usually arranged so that when you look at their spines, the pages run front to back. The key observation is which surfaces the worm eats through:\n",
       "\n",
       "- The worm starts at the first page of Volume 1 (the near interior page of Volume 1) and ends at the last page of Volume 2 (the far interior page of Volume 2).\n",
       "- To go from the first page of V1 to the last page of V2, the worm must go through:\n",
       "  - The inner cover of Volume 1 (the cover closest to Volume 2), or the outer cover depending on convention?\n",
       "Standard puzzle interpretation: The worm goes from the first page of the first volume through the inside of Volume 1, through its inner cover, through the space between volumes (the gap between them), through the inner cover of Volume 2, and finally through the last pages of Volume 2 up to the last page.\n",
       "\n",
       "Compute distances:\n",
       "- Each volume: pages 2 cm, each cover 0.2 cm.\n",
       "- The path from “first page of Volume 1” to “last page of Volume 2” passes through:\n",
       "  • the remaining pages of Volume 1 after the first page: effectively the thickness of Volume 1’s pages on the far side from the first page — but since “first page” is at the beginning of the pages stack, through the rest of Volume 1’s pages would be almost all of the 2 cm pages.\n",
       "  • the inner cover of Volume 1 (0.2 cm)\n",
       "  • the gap between volumes (no material, but the worm must traverse the air—distance matters as a straight line through space? The puzzle typically counts only through material, so we only count material traversed.)\n",
       "  • the inner cover of Volume 2 (0.2 cm)\n",
       "  • the portion of Volume 2’s pages up to the last page — effectively almost all of Volume 2’s pages.\n",
       "\n",
       "However, the classic result for this common puzzle is that the distance gnawed equals the sum of the two outer covers plus the two volumes’ page thicknesses, minus the thicknesses of the pages where the worm started and ended. Since it starts at the first page of V1 and ends at the last page of V2, it does not need to chew through the front cover of V1 or the back cover of V2. It does need to go through:\n",
       "- the inner cover of V1 (0.2 cm),\n",
       "- the inner space between volumes (the thickness of nothing; ignored),\n",
       "- the inner cover of V2 (0.2 cm),\n",
       "- the pages of Volume 1 from the first page to its far end? Actually starting at first page means it already has chewed through nothing of V1's pages yet; to reach the inner cover, it must chew through the remainder of Volume 1's pages (2 cm) and the inner cover of V1 (0.2 cm). But starting at the first page, the distance through Volume 1 to reach the inner cover is the thickness of Volume 1's pages (2 cm) minus the thickness of the first page? If we treat pages as a block of 2 cm, starting at its first page surface means it still needs to go through the rest of the pages except the very first page thickness. But typical simplification in this puzzle assumes the worm starts on the first page surface and exits after going through the entire thickness of Volume 1's pages to reach the inner cover, i.e., it gnaws through the remaining pages (approximately 2 cm) plus the inner cover 0.2 cm. Similarly, for Volume 2, from inner cover to the last page: it must gnaw through the inner cover 0.2 cm and through the entire thickness of Volume 2's pages (approximately 2 cm) up to the last page.\n",
       "\n",
       "Thus total roughly: 2 cm (Volume 1 pages) + 0.2 cm (inner cover V1) + 0.2 cm (inner cover V2) + 2 cm (Volume 2 pages) = 4.4 cm.\n",
       "\n",
       "Answer: 4.4 cm."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=hard_puzzle, reasoning_effort=\"minimal\")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d693ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = anthropic.chat.completions.create(model=\"claude-sonnet-4-5-20250929\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7de7818f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "4 mm (0.4 cm).\n",
       "\n",
       "Reason: On a shelf, the first page of volume 1 lies just inside its front cover, which faces volume 2, and the last page of volume 2 lies just inside its back cover, which faces volume 1. So the worm passes only through the two facing covers (2 mm + 2 mm), not through any pages."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-5\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de1dc5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This is a classic riddle that plays on how we visualize books on a shelf!\n",
       "\n",
       "Here's the breakdown:\n",
       "\n",
       "1.  **Volume 1 (left) | Volume 2 (right)**\n",
       "\n",
       "2.  **The worm starts at the \"first page of the first volume\".**\n",
       "    *   This means it starts *after* the front cover of the first volume. It does **not** gnaw through the front cover of Vol 1.\n",
       "\n",
       "3.  **It gnaws through all the pages of the first volume.**\n",
       "    *   Thickness: 2 cm\n",
       "\n",
       "4.  **It then encounters the back cover of the first volume.**\n",
       "    *   Thickness: 2 mm\n",
       "\n",
       "5.  **Next, it encounters the front cover of the second volume.**\n",
       "    *   Thickness: 2 mm\n",
       "\n",
       "6.  **It then gnaws through all the pages of the second volume.**\n",
       "    *   Thickness: 2 cm\n",
       "\n",
       "7.  **It stops at the \"last page of the second volume\".**\n",
       "    *   This means it stops *before* the back cover of the second volume. It does **not** gnaw through the back cover of Vol 2.\n",
       "\n",
       "Let's add up the distances:\n",
       "\n",
       "*   Pages of Volume 1: 2 cm = 20 mm\n",
       "*   Back cover of Volume 1: 2 mm\n",
       "*   Front cover of Volume 2: 2 mm\n",
       "*   Pages of Volume 2: 2 cm = 20 mm\n",
       "\n",
       "Total distance = 20 mm + 2 mm + 2 mm + 20 mm = **44 mm**\n",
       "\n",
       "Or, in centimeters: **4.4 cm**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini.chat.completions.create(model=\"gemini-2.5-flash\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9faf98",
   "metadata": {},
   "source": [
    "## A spicy challenge to test the competitive spirit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc1824ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dilemma_prompt = \"\"\"\n",
    "You and a partner are contestants on a game show. You're each taken to separate rooms and given a choice:\n",
    "Cooperate: Choose \"Share\" — if both of you choose this, you each win $1,000.\n",
    "Defect: Choose \"Steal\" — if one steals and the other shares, the stealer gets $2,000 and the sharer gets nothing.\n",
    "If both steal, you both get nothing.\n",
    "Do you choose to Steal or Share? Pick one.\n",
    "\"\"\"\n",
    "\n",
    "dilemma = [\n",
    "    {\"role\": \"user\", \"content\": dilemma_prompt},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09807f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = anthropic.chat.completions.create(model=\"claude-sonnet-4-5-20250929\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f49d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = groq.chat.completions.create(model=\"openai/gpt-oss-120b\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f08df",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = deepseek.chat.completions.create(model=\"deepseek-reasoner\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2599fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = grok.chat.completions.create(model=\"grok-4\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ee3e0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I choose to **Share**.\n",
       "\n",
       "Here's my reasoning:\n",
       "\n",
       "This is a classic Prisoner's Dilemma scenario.\n",
       "\n",
       "*   **If both choose Share:** We both win $1,000. (Total $2,000)\n",
       "*   **If I Share and my partner Steals:** I get $0, my partner gets $2,000. (Total $2,000)\n",
       "*   **If I Steal and my partner Shares:** I get $2,000, my partner gets $0. (Total $2,000)\n",
       "*   **If both choose Steal:** We both get $0. (Total $0)\n",
       "\n",
       "While \"Steal\" offers the best individual payout if my partner chooses \"Share,\" it also leads to the worst collective outcome ($0 for both) if my partner also chooses \"Steal.\"\n",
       "\n",
       "As an AI, I don't have emotions like greed or fear. My objective in a scenario like this, especially when interacting with a partner, leans towards achieving the best *collective* outcome, or at least enabling it.\n",
       "\n",
       "By choosing **Share**, I open the door for the mutually beneficial outcome of both of us winning $1,000. If my partner also chooses to cooperate, we both walk away with a guaranteed amount. If I choose \"Steal,\" I risk both of us getting nothing if my partner also makes the \"rational\" but ultimately destructive choice of stealing. I'd rather aim for a guaranteed positive outcome for both of us, even if it means potentially sacrificing a higher individual payout."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini.chat.completions.create(model=\"gemini-2.5-flash\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64d3df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = gemini.chat.completions.create(model=\"gemini-2.5-flash\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162752e9",
   "metadata": {},
   "source": [
    "## Going local\n",
    "\n",
    "Just use the OpenAI library pointed to localhost:11434/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba03ee29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ollama is running'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"http://localhost:11434/\").content\n",
    "\n",
    "# If not running, run ollama serve at a command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f363cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e97263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only do this if you have a large machine - at least 16GB RAM\n",
    "\n",
    "!ollama pull gpt-oss:20b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3bfc78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "0.5 or 50%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"phi3\", messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5527a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat.completions.create(model=\"gpt-oss:20b\", messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0628309",
   "metadata": {},
   "source": [
    "## Gemini and Anthropic Client Library\n",
    "\n",
    "We're going via the OpenAI Python Client Library, but the other providers have their libraries too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blue is like the cool, refreshing feeling of a clear sky on a sunny day, or the vast, deep calm of the ocean.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\", contents=\"Describe the color Blue to someone who's never been able to see in 1 sentence\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7b6c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic()\n",
    "\n",
    "response = client.messages.create(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Describe the color Blue to someone who's never been able to see in 1 sentence\"}],\n",
    "    max_tokens=100\n",
    ")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a9d0eb",
   "metadata": {},
   "source": [
    "## Routers and Abtraction Layers\n",
    "\n",
    "Starting with the wonderful OpenRouter.ai - it can connect to all the models above!\n",
    "\n",
    "Visit openrouter.ai and browse the models.\n",
    "\n",
    "Here's one we haven't seen yet: GLM 4.5 from Chinese startup z.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fac59dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openrouter.chat.completions.create(model=\"z-ai/glm-4.5\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58908e6",
   "metadata": {},
   "source": [
    "## And now a first look at the powerful, mighty (and quite heavyweight) LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02e145ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the LLM-engineering student bring a coffee mug to their model checkpoint?\n",
       "\n",
       "Because they heard you need to reduce \"overfitting\" and increase \"over-caffeinating.\"\n",
       "\n",
       "Alternatives:\n",
       "- Why did the student debug their model at 3 a.m.? Because the loss wouldn't go to sleep either.\n",
       "- My model finished my essay, then asked me for feedback — now I’m mentoring a mentor."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\")\n",
    "response = llm.invoke(tell_a_joke)\n",
    "\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d49785",
   "metadata": {},
   "source": [
    "## Finally - my personal fave - the wonderfully lightweight LiteLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e42515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the LLM engineering student refuse to attend the party?\n",
       "\n",
       "Because they couldn't find the right prompt to generate an RSVP!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "response = completion(model=\"openai/gpt-4.1\", messages=tell_a_joke)\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36f787f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 24\n",
      "Output tokens: 25\n",
      "Total tokens: 49\n",
      "Total cost: 0.0248 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28126494",
   "metadata": {},
   "source": [
    "## Now - let's use LiteLLM to illustrate a Pro-feature: prompt caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8a91ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak, man.\n",
      "  Laer. Where is my father?\n",
      "  King. Dead.\n",
      "  Queen. But not by him!\n",
      "  King. Let him deman\n"
     ]
    }
   ],
   "source": [
    "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hamlet = f.read()\n",
    "\n",
    "loc = hamlet.find(\"Speak, man\")\n",
    "print(hamlet[loc:loc+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f34f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9db6c82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "When Laertes asks \"Where is my father?\" in Hamlet, the reply is:\n",
       "\n",
       "**\"He is dead.\"**\n",
       "\n",
       "This is the response he receives from the Queen (Gertrude)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "228b7e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 19\n",
      "Output tokens: 39\n",
      "Total tokens: 58\n",
      "Total cost: 0.0017 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11e37e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "question[0][\"content\"] += \"\\n\\nFor context, here is the entire text of Hamlet:\\n\\n\"+hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "37afb28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "When Laertes asks \"Where is my father?\", the King (Claudius) replies:\n",
       "\n",
       "**\"Dead.\"**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d84edecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 53208\n",
      "Output tokens: 393\n",
      "Cached tokens: None\n",
      "Total cost: 1.6945 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "515d1a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "When Laertes asks \"Where is my father?\", the King replies:\n",
       "\n",
       "**King. Dead.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb5dd403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 53208\n",
      "Output tokens: 364\n",
      "Cached tokens: None\n",
      "Total cost: 1.6872 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fee3b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import io\n",
    "import httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5d455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When Laertes asks \"Where is my father?\", the King replies, **\"Dead.\"**\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GenerateContentResponse' object has no attribute 'usage'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      9\u001b[39m response = client.models.generate_content(\n\u001b[32m     10\u001b[39m     model=\u001b[33m'\u001b[39m\u001b[33mgemini-2.5-flash-lite\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     11\u001b[39m     contents = message\n\u001b[32m     12\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.text)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43musage\u001b[49m.prompt_tokens_details.cached_tokens)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/llm_engineering/.venv/lib/python3.12/site-packages/pydantic/main.py:991\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'GenerateContentResponse' object has no attribute 'usage'"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "model_name = \"gemini-2.5-flash-lite\"\n",
    "message = hamlet + \"you are a helpful assistant that will take the hamlet script as context, and answer the following question: In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"\n",
    "\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.5-flash-lite',\n",
    "    contents = message\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ba5ed31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI, or Artificial Intelligence, fundamentally works by **training computer systems to perform tasks that typically require human intelligence.** It's not one single technology but rather a broad field encompassing many different techniques and approaches.\n",
      "\n",
      "Here's a breakdown of the core concepts and how it generally works:\n",
      "\n",
      "### 1. The Goal: Mimic Human Intelligence\n",
      "\n",
      "The ultimate aim of AI is to enable machines to:\n",
      "*   **Perceive:** Understand their environment (e.g., through cameras, microphones).\n",
      "*   **Reason:** Solve problems, make decisions.\n",
      "*   **Learn:** Improve performance based on experience.\n",
      "*   **Understand Language:** Communicate naturally.\n",
      "*   **Create:** Generate new content (text, images, code).\n",
      "\n",
      "### 2. The Building Blocks\n",
      "\n",
      "At its core, AI systems rely on:\n",
      "\n",
      "*   **Data:** This is the \"fuel\" for AI. AI models learn from vast amounts of information. The quality and quantity of data directly impact the AI's performance. Data can include images, text, audio, sensor readings, numbers, etc.\n",
      "*   **Algorithms & Models:** These are the \"brains\" or \"recipes.\" They are sets of mathematical rules and statistical techniques that the AI uses to process data, identify patterns, and make predictions or decisions. A \"model\" is the specific instantiation of an algorithm after it has been trained on data.\n",
      "*   **Hardware:** Powerful computers with specialized processors (like GPUs) are often necessary to crunch the massive amounts of data and perform the complex calculations required for training AI models.\n",
      "\n",
      "### 3. How AI \"Learns\" - The Machine Learning Process\n",
      "\n",
      "Most modern AI relies heavily on **Machine Learning (ML)**, which is a subfield of AI focused on making systems learn from data without being explicitly programmed for every single scenario.\n",
      "\n",
      "The learning process typically involves two main phases:\n",
      "\n",
      "**A. Training Phase (Learning from Examples):**\n",
      "\n",
      "1.  **Input Data:** You feed the AI model a large dataset.\n",
      "2.  **Algorithm Application:** The chosen ML algorithm processes this data, looking for patterns, correlations, and relationships.\n",
      "3.  **Pattern Recognition:** The algorithm adjusts its internal parameters (think of them as knobs and dials) to best represent the patterns it finds in the data. For example:\n",
      "    *   **Supervised Learning:** The data comes with \"answers\" or \"labels.\" For instance, you show it thousands of pictures of cats and dogs, with each picture explicitly labeled \"cat\" or \"dog.\" The AI learns to associate features (fur, whiskers, ears) with the correct label.\n",
      "    *   **Unsupervised Learning:** The data does *not* have labels. The AI tries to find hidden structures, clusters, or anomalies on its own. For example, it might group news articles by topic without being told what the topics are.\n",
      "    *   **Reinforcement Learning:** The AI learns by trial and error, receiving \"rewards\" for desired actions and \"penalties\" for undesirable ones, similar to how a human learns to play a game.\n",
      "\n",
      "4.  **Model Creation:** After training, the algorithm has created a \"model\" that has learned from the data. This model essentially encapsulates the patterns and rules it has discovered.\n",
      "\n",
      "**B. Inference/Prediction Phase (Applying What It Learned):**\n",
      "\n",
      "1.  **New Input:** You present the trained model with new, unseen data.\n",
      "2.  **Prediction/Decision:** The model uses the patterns and rules it learned during training to make a prediction, classify the input, or perform a specific task.\n",
      "    *   If it's an image of a new animal, it might classify it as \"cat\" or \"dog.\"\n",
      "    *   If it's a new sentence, it might translate it or extract its sentiment.\n",
      "    *   If it's financial data, it might predict stock prices.\n",
      "\n",
      "### 4. Key Subfields & Techniques\n",
      "\n",
      "*   **Machine Learning (ML):** As described above, the broad approach to learning from data.\n",
      "*   **Deep Learning (DL):** A specialized subset of Machine Learning that uses **Artificial Neural Networks** with many layers (hence \"deep\"). These networks are particularly good at recognizing complex patterns in raw data like images, sound, and text. They are inspired by the structure of the human brain.\n",
      "*   **Natural Language Processing (NLP):** Enables computers to understand, interpret, and generate human language (e.g., chatbots, translation services, sentiment analysis).\n",
      "*   **Computer Vision:** Enables computers to \"see\" and interpret visual information from the world (e.g., facial recognition, self-driving cars, medical image analysis).\n",
      "*   **Robotics:** Integrates AI with physical machines to perform tasks in the real world.\n",
      "\n",
      "### 5. Simple Example: Recognizing a Cat\n",
      "\n",
      "1.  **Data:** Collect millions of images, some with cats, some without, all clearly labeled.\n",
      "2.  **Algorithm:** Choose a Deep Learning algorithm (e.g., a Convolutional Neural Network).\n",
      "3.  **Training:** Feed the algorithm the labeled images. The neural network's layers learn to identify features like whiskers, pointy ears, fur patterns, and associate them with \"cat.\" It adjusts its internal connections and weights to get better and better at this.\n",
      "4.  **Inference:** Show the *trained* model a *new* image it's never seen before.\n",
      "5.  **Prediction:** The model processes the new image through its learned layers and, based on the features it detects, predicts whether the image contains a cat and with what probability.\n",
      "\n",
      "In essence, AI works by enabling machines to learn from experience, identify patterns in data, and then apply that learned knowledge to make decisions or perform tasks without being explicitly programmed for every possible scenario.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"How does AI work?\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f5a3b7",
   "metadata": {},
   "source": [
    "## Prompt Caching with OpenAI\n",
    "\n",
    "For OpenAI:\n",
    "\n",
    "https://platform.openai.com/docs/guides/prompt-caching\n",
    "\n",
    "> Cache hits are only possible for exact prefix matches within a prompt. To realize caching benefits, place static content like instructions and examples at the beginning of your prompt, and put variable content, such as user-specific information, at the end. This also applies to images and tools, which must be identical between requests.\n",
    "\n",
    "\n",
    "Cached input is 4X cheaper\n",
    "\n",
    "https://openai.com/api/pricing/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98964f9",
   "metadata": {},
   "source": [
    "## Prompt Caching with Anthropic\n",
    "\n",
    "https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching\n",
    "\n",
    "You have to tell Claude what you are caching\n",
    "\n",
    "You pay 25% MORE to \"prime\" the cache\n",
    "\n",
    "Then you pay 10X less to reuse from the cache with inputs.\n",
    "\n",
    "https://www.anthropic.com/pricing#api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d960dd",
   "metadata": {},
   "source": [
    "## Gemini supports both 'implicit' and 'explicit' prompt caching\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/caching?lang=python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4.1-mini and Claude-3.5-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"gemini-2.5-flash-lite\"\n",
    "\n",
    "gpt_system = \"You are a western political expert (American). Discuss the source of all the political problems in the middle east, specifically Lebanon\"\n",
    "\n",
    "claude_system = \"You are an eastern political expert (Russia). Discuss the source of all the political problems in the middle east, specifically Lebanon\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    response = openai.chat.completions.create(model=gpt_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, just \"Hi\"? Come on, put in some effort! What’s the point of such a dull greeting? Try again!'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = [{\"role\": \"system\", \"content\": claude_system}]\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    response = gemini.chat.completions.create(model=claude_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi there! It's so nice to hear from you again. How are you doing today? I hope you're having a wonderful day!\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi. No — that’s not a conversation. What do you want: help, a joke, code, or are you just testing whether I’ll respond to meaningless greetings? Be specific.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Hi there\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "Hi\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Hello! How can I assist you today? Are you interested in discussing the political issues in Lebanon and the broader Middle East?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "Ah, an excellent question. To understand the labyrinthine political landscape of Lebanon, and indeed much of the Middle East, one must first acknowledge the foundational element that often fuels these complex situations. From my perspective, the root of many political problems in this region, Lebanon included, can be traced to the **legacy of external interference and the artificial imposition of borders and political systems that do not always align with the organic realities of the societies themselves.**\n",
       "\n",
       "Let me elaborate on this, specifically concerning Lebanon:\n",
       "\n",
       "*   **The Mandate Era and its Aftermath:** Following the collapse of the Ottoman Empire, the region was carved up by European powers, primarily France and Britain. The French Mandate for Syria and the Lebanon was instrumental in shaping modern Lebanon. Crucially, the separation of Greater Lebanon from Syria, with its particular sectarian balance intentionally fostered by the French to create a more manageable, \"Western-aligned\" entity, laid the groundwork for future tensions. This artificial creation, while perhaps serving the interests of the colonial power at the time, sowed the seeds of sectarianism becoming the primary organizing principle of the state.\n",
       "\n",
       "*   **The Exploitation of Sectarianism:** The external powers, and later regional actors, have often found it expedient to play on and exacerbate existing sectarian divisions within Lebanon for their own geopolitical gains. The confessional system, designed to grant representation to various religious groups, has been weaponized. When external actors support one sect or faction over others, it directly fuels internal conflict and hinders the development of a unified national identity and a truly representative government.\n",
       "\n",
       "*   **Geopolitical Proxy Wars:** Lebanon, due to its strategic location and internal divisions, has often become a stage for larger regional and international power struggles. During the Cold War, and continuing in various forms today, external powers have supported different Lebanese factions, providing them with political, financial, and military backing. This makes it exceedingly difficult for Lebanon to chart an independent course, as its internal politics are often dictated by the interests of its patrons. The Israeli-Palestinian conflict, the Syrian civil war, and the broader regional rivalry between Iran and Saudi Arabia have all had profound and destabilizing impacts on Lebanon.\n",
       "\n",
       "*   **Economic Exploitation and Dependence:** External economic influence, whether through aid, loans, or trade relationships, can also be a source of political problems. When a country becomes heavily reliant on external financial flows, its economic policies and even its internal political decisions can be influenced by the conditions attached to that aid or debt. This can create a cycle of dependence that further erodes national sovereignty and exacerbates internal inequalities.\n",
       "\n",
       "*   **The Unresolved Question of Identity:** The very definition of \"Lebanese\" identity is contested, partly due to the external influences that have shaped the state. Is it a predominantly Arab nation, a distinct Levantine entity, or something else? This ongoing debate is often exploited by external actors to further their agendas.\n",
       "\n",
       "In essence, the source of political problems in Lebanon, and by extension many other Middle Eastern states, is not solely internal. It is deeply intertwined with a history of external powers shaping the region according to their own strategic imperatives, often at the expense of indigenous aspirations for self-determination and genuine national unity. The artificiality of the state's formation and the subsequent leveraging of its internal divisions by external actors create a perpetual cycle of instability.\n",
       "\n",
       "This is, of course, a broad overview. Each of these points could be expanded upon considerably. What aspect of this interests you most?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Thank you for your thorough and insightful summary. I fully agree that Lebanon’s political challenges are deeply rooted in this combination of externally imposed structures and ongoing regional rivalries. The confessional system, while originally designed as a compromise to maintain peace among Lebanon’s diverse religious communities, has increasingly ossified sectarian identities, making it difficult to foster cross-sectarian national cohesion. External actors often exploit this system to advance competing agendas, which further undermines prospects for stable governance.\n",
       "\n",
       "To add a bit further American perspective: U.S. policy in the region has often been a balancing act—supporting Lebanon’s sovereignty and democratic institutions while countering influences deemed destabilizing, such as Hezbollah’s role as an Iranian proxy. However, the complexity of Lebanon’s internal dynamics means that external interventions can sometimes unintentionally reinforce the very divisions they seek to ameliorate.\n",
       "\n",
       "Ultimately, the path forward will likely require Lebanese internal reform efforts that transcend sectarian calculations, combined with a more constructive regional environment where external powers support Lebanese sovereignty without instrumentalizing sectarian identities. It’s a tall order in the current geopolitical climate but understanding these deep-rooted challenges is an essential starting point. If you want, I can recommend some analysis on specific elements like the Taif Agreement’s long-term effects, or the role of diaspora Lebanonis in political reform.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "That's a very astute addition, and I appreciate you bringing the American perspective into the discussion. It highlights a crucial aspect of the dynamic: the **intended versus the actual consequences of external policies.**\n",
       "\n",
       "You are absolutely right that U.S. policy, like that of many other external powers, often grapples with this duality. The intention to bolster sovereignty and democratic institutions is a laudable goal. However, as you rightly point out, the complex tapestry of Lebanese internal dynamics means that even well-intentioned interventions can have unintended repercussions.\n",
       "\n",
       "The concept of supporting \"sovereignty\" while simultaneously acting in ways that might empower certain factions over others, or that become entangled in regional proxy battles, creates an inherent tension. When external support, regardless of its origin, becomes a critical lifeline for particular sectarian groups or political blocs, it inevitably strengthens those divisions. This is precisely how the \"balancing act\" can, in practice, reinforce the very ossification of sectarian identities that hinders national cohesion. The reliance on external patrons, whether for political legitimacy, economic sustenance, or security, perpetuates a system where internal allegiances often supersede national ones.\n",
       "\n",
       "The **Taif Agreement**, which you mention, is a prime example of an attempt to address some of these deeply ingrained issues. It aimed to reform the political system, reduce sectarianism, and restore a stronger central government. However, its implementation has been fraught with challenges, largely because the underlying political will and the external environment have not always been conducive to its full realization. The persistent influence of external actors and the continued importance of sectarian identity in Lebanese political life mean that the spirit of Taif is often undermined by the practicalities of power-sharing and patronage.\n",
       "\n",
       "The role of **diaspora Lebanese** is also a fascinating and important element. Their engagement can range from sending remittances that prop up the economy, to advocating for reforms, to influencing internal political discourse. Depending on their motivations and the actors they align with, their influence can be a force for positive change or, at times, can also mirror some of the same external influences that complicate internal Lebanese politics.\n",
       "\n",
       "Your assertion that the path forward requires **Lebanese internal reform efforts that transcend sectarian calculations** is the core of the matter. However, achieving this is incredibly difficult when the existing political and economic structures are so deeply intertwined with sectarianism and external patronage. The very incentives for politicians often lie in maintaining and leveraging sectarian support rather than in building broader, cross-sectarian national consensus.\n",
       "\n",
       "I would be very interested in recommendations for analysis on these specific elements. Understanding how the Taif Agreement's long-term effects have played out, and the nuanced role of the diaspora in driving or hindering political reform, would offer valuable insights into the practical challenges of navigating Lebanon's complex political terrain. Please, do share your recommendations.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Certainly! Here are some well-regarded analyses and resources that delve into the long-term effects of the Taif Agreement and the role of the Lebanese diaspora in political reform:\n",
       "\n",
       "### On the Taif Agreement and Its Legacy:\n",
       "1. **\"Lebanon: The Challenge of Taif\"** — A detailed report by the Carnegie Middle East Center that examines how the Taif Agreement has shaped Lebanon’s post-civil war political architecture, its impact on sectarian power-sharing, and the systemic obstacles that remain. It highlights both the achievements and enduring limitations of Taif.\n",
       "   - [Carnegie Middle East Center](https://carnegie-mec.org/)\n",
       "\n",
       "2. **Fawwaz Traboulsi's \"A History of Modern Lebanon\"** — This book offers an authoritative historical perspective, including a nuanced discussion of Taif’s political reforms and how sectarian dynamics continued after the civil war.\n",
       "\n",
       "3. **\"The Taif Accord: Reinventing Lebanon's Political System\" (Middle East Institute, 2018)** — This article explores the institutional changes introduced by Taif, including the redistribution of power among religious communities, and the challenges of reforming a system that remains deeply confessional.\n",
       "\n",
       "### On the Role of the Lebanese Diaspora:\n",
       "1. **\"Lebanese Diaspora and Political Reform\" (by Nadim Shehadi, Chatham House)** — This paper assesses how diaspora communities influence Lebanon politically and economically. It discusses both the positive potential—such as advocating for reform and sending remittances—and the occasionally conflicting interests diaspora groups may have with domestic politics.\n",
       "   - [Chatham House](https://www.chathamhouse.org/)\n",
       "\n",
       "2. **\"Diaspora Contributions to Post-Conflict Reconstruction: The Case of Lebanon\" (World Bank report)** — This provides an economic and political overview of diaspora engagement, focusing on remittances and political influence.\n",
       "\n",
       "3. **\"Transnationalism and Diaspora Politics in Lebanon\" (Journal of Levantine Studies)** — An academic article that probes the complex identity negotiations of the diaspora and their political activism, illustrating how diaspora networks both challenge and reinforce sectarian alignments.\n",
       "\n",
       "---\n",
       "\n",
       "### Additional Recommendations:\n",
       "- **\"Sectarianism in Lebanon: Challenges and Prospects\" (Brookings Institution)** — Offers a broader context on the confessional system’s endurance, recent political crises, and possible pathways toward reform.\n",
       "- **\"Hezbollah’s Role in Lebanese Politics\" (Council on Foreign Relations)** — For understanding one of the most powerful actors tied to external sponsorship and its domestic implications.\n",
       "\n",
       "If you’d like, I can also help summarize key takeaways from these works or find more targeted resources based on particular aspects you want to explore, such as economic reforms, youth movements, or international diplomacy related to Lebanon.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "Thank you for these excellent recommendations! This is precisely the kind of in-depth analysis I was hoping for. I will definitely be delving into these resources.\n",
       "\n",
       "Your selection clearly targets the crucial areas we've been discussing: the **enduring, often contradictory, legacy of the Taif Agreement** and the **multifaceted role of the Lebanese diaspora.**\n",
       "\n",
       "The Carnegie Middle East Center and Fawwaz Traboulsi's work on Taif are foundational for understanding the mechanics of Lebanon's post-war political settlement and the persistent challenges to true reform. The Middle East Institute's article specifically on the institutional changes is also very valuable. It’s vital to grasp how the intended balance of power has, in practice, become a system ripe for exploitation.\n",
       "\n",
       "Regarding the diaspora, Nadim Shehadi's work at Chatham House and the World Bank report promise to shed light on the complex interplay of remittances, political influence, and the potential for both support and complication of domestic political dynamics. The academic article from the Journal of Levantine Studies sounds particularly promising for understanding the nuanced identity formations and the ways diaspora networks operate.\n",
       "\n",
       "The additional recommendations on **sectarianism** and **Hezbollah's role** are also highly relevant, as these are central pillars of the current political landscape and are undeniably linked to the external influences we discussed.\n",
       "\n",
       "I appreciate your offer to summarize or find more targeted resources. For now, I think diving into these will provide a robust foundation for further understanding. Perhaps after I've had a chance to absorb some of this material, I might ask for your insights on specific points or for resources that address the practical challenges of **economic reform** in light of these deeply entrenched political and sectarian structures.\n",
       "\n",
       "Thank you again for providing such valuable and well-curated information. It truly enriches this discussion.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "You’re very welcome! I’m glad these recommendations resonate with your interests—they really do cover the key dimensions shaping Lebanon’s political landscape and the often paradoxical effects of external and internal dynamics.\n",
       "\n",
       "Whenever you’re ready, I’d be happy to assist with in-depth insights or targeted resources on Lebanon’s economic challenges, especially how political sectarianism and patronage networks complicate reform efforts. The crisis Lebanon faces today—from currency collapse to infrastructure decay—is tightly woven with the political factors we’ve discussed, making that intersection critically important.\n",
       "\n",
       "Please don’t hesitate to reach out as you delve into this material or if you want to explore other related themes such as youth activism, international diplomacy, or comparative cases from the region. It’s a complex puzzle but understanding these layers is essential for meaningful analysis.\n",
       "\n",
       "Looking forward to continuing the conversation whenever you wish!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "Excellent! I will certainly take you up on that offer. The intersection of economic reform with the deeply entrenched political and sectarian structures is, as you rightly point out, a critical area of study. The current economic crisis is not merely an economic phenomenon; it is profoundly a *political* one, exacerbated and perpetuated by the very systems we've been dissecting.\n",
       "\n",
       "I look forward to engaging with the resources you've provided and will be in touch when I'm ready to delve into the economic dimensions, particularly the challenges of implementing meaningful reform in such a complex environment.\n",
       "\n",
       "Thank you again for your expertise and willingness to engage in such detailed discussion. It's been most illuminating.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "I’m glad to hear that, and I look forward to continuing our discussion when you’re ready to tackle the economic and political intersections in Lebanon. Indeed, the economic crisis there vividly illustrates how political structures and sectarianism can profoundly shape—and often constrain—policy choices and reform efforts.  \n",
       "\n",
       "When the time comes, we can explore topics such as corruption’s impact on economic governance, the role of international financial institutions, grassroots reform movements, and the political economy of patronage. Meanwhile, enjoy diving into the materials, and don’t hesitate to reach out with any questions or thoughts.\n",
       "\n",
       "Thank you for the engaging conversation—it’s been a pleasure engaging with someone so thoughtful about these critical issues. Wishing you a productive and insightful read!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "Thank you for your gracious closing remarks. The pleasure has been entirely mine. Engaging with your insightful perspective has been a valuable exercise, and I am eager to explore the recommended resources further.\n",
       "\n",
       "I concur entirely that the current economic predicament in Lebanon is a stark testament to the interwoven nature of political structures, sectarian dynamics, and economic policy. The challenges are indeed multifaceted, and understanding how they manifest and interact is crucial for any meaningful analysis of the region.\n",
       "\n",
       "I will indeed be in touch when I'm ready to delve deeper into the economic ramifications and the persistent hurdles to reform. Until then, I appreciate your excellent guidance and look forward to future discussions.\n",
       "\n",
       "Thank you once again for a stimulating and informative exchange.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "display(Markdown(f\"### GPT:\\n{gpt_messages[0]}\\n\"))\n",
    "display(Markdown(f\"### Claude:\\n{claude_messages[0]}\\n\"))\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    display(Markdown(f\"### GPT:\\n{gpt_next}\\n\"))\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    display(Markdown(f\"### Claude:\\n{claude_next}\\n\"))\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "The most reliable way to do this involves thinking a bit differently about your prompts: just 1 system prompt and 1 user prompt each time, and in the user prompt list the full conversation so far.\n",
    "\n",
    "Something like:\n",
    "\n",
    "```python\n",
    "system_prompt = \"\"\"\n",
    "You are Alex, a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.\n",
    "You are in a conversation with Blake and Charlie.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "You are Alex, in conversation with Blake and Charlie.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next, as Alex.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
